# High-Level Design: eBay Search MCP Server

**Document Version**: 1.0
**Date**: 2025-11-13
**Status**: Design Phase
**Author**: System Design
**Technology**: Rust + MCP + Headless Browser

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [System Overview](#system-overview)
3. [Architecture Design](#architecture-design)
4. [Component Design](#component-design)
5. [Data Models](#data-models)
6. [MCP Interface Design](#mcp-interface-design)
7. [Configuration Management](#configuration-management)
8. [Headless Browser Integration](#headless-browser-integration)
9. [Search Management System](#search-management-system)
10. [Error Handling Strategy](#error-handling-strategy)
11. [Security Considerations](#security-considerations)
12. [Performance Optimization](#performance-optimization)
13. [Technology Stack](#technology-stack)
14. [Implementation Plan](#implementation-plan)
15. [Testing Strategy](#testing-strategy)
16. [Deployment Considerations](#deployment-considerations)
17. [Future Enhancements](#future-enhancements)
18. [Appendices](#appendices)

---

## Executive Summary

### Purpose

This document describes the high-level design for an MCP (Model Context Protocol) server that enables AI assistants to search eBay using a headless browser. The server provides intelligent eBay search capabilities with support for saved search phrases, configurable search parameters, and structured result retrieval.

### Key Objectives

1. **Intelligent eBay Search**: Provide AI-powered eBay search capabilities through MCP
2. **Search Management**: Enable users to save, manage, and reuse search phrases
3. **Headless Automation**: Use browser automation for robust eBay scraping
4. **Rust Performance**: Leverage Rust's performance and safety guarantees
5. **MCP Compliance**: Full compliance with MCP specification and best practices

### Core Features

- **Headless Browser Search**: Automated eBay searches using Chrome/Chromium
- **Saved Search Phrases**: Persistent storage of frequently used searches
- **Search History**: Track and retrieve past search results
- **Flexible Filtering**: Support eBay's advanced search parameters
- **Result Caching**: Cache search results to improve performance
- **Real-time Scraping**: Extract current listings, prices, and metadata

### Success Criteria

- Sub-10 second response time for typical searches
- Support for 100+ saved search phrases
- Reliable parsing of eBay search results (>95% success rate)
- Zero runtime panics through Rust's safety features
- Full MCP protocol compliance

---

## System Overview

### High-Level Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                      MCP Client (Claude)                     │
│                                                              │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐     │
│  │   Search     │  │    Manage    │  │    View      │     │
│  │   eBay       │  │   Phrases    │  │   Results    │     │
│  └──────────────┘  └──────────────┘  └──────────────┘     │
└───────────────────────────┬─────────────────────────────────┘
                            │ JSON-RPC 2.0 / stdio
                            ▼
┌─────────────────────────────────────────────────────────────┐
│              eBay Search MCP Server (Rust)                   │
│                                                              │
│  ┌────────────────────────────────────────────────────┐    │
│  │              MCP Protocol Layer                     │    │
│  │  - Tool Handlers  - Resource Handlers              │    │
│  │  - Prompt Templates  - Error Management            │    │
│  └────────────────────────────────────────────────────┘    │
│                            │                                 │
│  ┌────────────────────────────────────────────────────┐    │
│  │              Search Management Layer                │    │
│  │  - Phrase Storage  - Search Execution              │    │
│  │  - Result Caching  - History Tracking              │    │
│  └────────────────────────────────────────────────────┘    │
│                            │                                 │
│  ┌────────────────────────────────────────────────────┐    │
│  │           Headless Browser Layer                    │    │
│  │  - Browser Pool  - Page Navigation                 │    │
│  │  - Element Extraction  - Anti-Detection            │    │
│  └────────────────────────────────────────────────────┘    │
│                            │                                 │
│  ┌────────────────────────────────────────────────────┐    │
│  │              Data Persistence Layer                 │    │
│  │  - Config File (TOML)  - Search History (SQLite)   │    │
│  │  - Cache Store (Disk)                              │    │
│  └────────────────────────────────────────────────────┘    │
└───────────────────────────┬─────────────────────────────────┘
                            │
                            ▼
                    ┌──────────────┐
                    │  eBay.com    │
                    └──────────────┘
```

### Component Interaction Flow

1. **MCP Client → Server**: User requests eBay search via AI assistant
2. **Server → Search Manager**: Parse request and retrieve saved phrases if applicable
3. **Search Manager → Browser**: Execute search with headless browser
4. **Browser → eBay**: Navigate to eBay and perform search
5. **eBay → Browser**: Return search results page
6. **Browser → Parser**: Extract structured data from HTML
7. **Parser → Cache**: Store results in cache
8. **Server → Client**: Return formatted results via MCP

### Key Design Principles

1. **Separation of Concerns**: Clear layer boundaries and responsibilities
2. **Fail-Safe Operation**: Graceful degradation and comprehensive error handling
3. **Performance First**: Async operations, connection pooling, intelligent caching
4. **Type Safety**: Leverage Rust's type system to prevent errors at compile time
5. **Configurability**: Extensive configuration options without code changes
6. **Observability**: Comprehensive logging and monitoring capabilities

---

## Architecture Design

### Layer Architecture

#### 1. MCP Protocol Layer

**Responsibilities**:
- Implement MCP server protocol (JSON-RPC 2.0)
- Handle tool calls, resource requests, and prompt retrieval
- Manage client sessions and lifecycle
- Protocol-level error handling and validation

**Key Modules**:
- `mcp_server`: Main server implementation
- `protocol_handler`: JSON-RPC message handling
- `capability_negotiation`: MCP capability exchange
- `transport`: stdio transport implementation

#### 2. Search Management Layer

**Responsibilities**:
- Manage saved search phrases
- Execute search operations
- Track search history
- Cache management and invalidation
- Search result aggregation

**Key Modules**:
- `search_manager`: Core search orchestration
- `phrase_store`: Saved phrase persistence
- `search_executor`: Search execution logic
- `result_cache`: Result caching system
- `history_tracker`: Search history management

#### 3. Headless Browser Layer

**Responsibilities**:
- Browser lifecycle management (launch, pool, shutdown)
- Page navigation and interaction
- Element extraction and scraping
- Anti-detection measures
- Screenshot capture for debugging

**Key Modules**:
- `browser_pool`: Connection pool management
- `page_controller`: Page navigation and control
- `element_extractor`: DOM element extraction
- `scraper`: eBay-specific scraping logic
- `anti_detection`: User agent rotation, timing randomization

#### 4. Data Persistence Layer

**Responsibilities**:
- Configuration file management (TOML)
- Database operations (SQLite)
- Cache storage and retrieval
- Data migration and versioning

**Key Modules**:
- `config_manager`: TOML configuration handling
- `database`: SQLite operations
- `cache_store`: File-based cache management
- `migration`: Schema versioning

### Module Dependency Graph

```
┌─────────────────┐
│   main.rs       │
└────────┬────────┘
         │
    ┌────▼────┐
    │ server  │
    └────┬────┘
         │
    ┌────▼──────────────────────────┐
    │                               │
┌───▼──────┐              ┌─────────▼────────┐
│ protocol │              │ search_manager   │
└────┬─────┘              └─────────┬────────┘
     │                              │
     │                    ┌─────────┼──────────┐
     │                    │         │          │
     │            ┌───────▼──┐  ┌──▼─────┐ ┌──▼────────┐
     │            │ browser  │  │ cache  │ │ database  │
     │            └──────────┘  └────────┘ └───────────┘
     │                    │
┌────▼──────┐    ┌───────▼──────┐
│ transport │    │  scraper     │
└───────────┘    └──────────────┘
```

### Concurrency Model

**Async Runtime**: Tokio-based async runtime for all I/O operations

**Thread Pools**:
- **Main async pool**: Handle MCP requests and coordination
- **Browser pool**: Manage multiple browser instances
- **Blocking pool**: Handle synchronous operations (file I/O, SQLite)

**Synchronization**:
- `Arc<RwLock<T>>` for shared state (config, cache)
- `Mutex<T>` for exclusive access (database writes)
- Channels (tokio::mpsc) for inter-component communication

---

## Component Design

### 1. MCP Server Component

**File**: `src/server/mod.rs`

**Responsibilities**:
- Initialize and run the MCP server
- Handle stdio transport
- Route requests to appropriate handlers
- Manage server lifecycle

**Key Structures**:

```rust
pub struct EbayMcpServer {
    /// Server configuration
    config: Arc<RwLock<ServerConfig>>,

    /// Search manager instance
    search_manager: Arc<SearchManager>,

    /// Browser pool
    browser_pool: Arc<BrowserPool>,

    /// Server state
    state: ServerState,
}

#[derive(Debug)]
pub struct ServerConfig {
    pub server_name: String,
    pub server_version: String,
    pub browser_config: BrowserConfig,
    pub cache_config: CacheConfig,
    pub database_path: PathBuf,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ServerState {
    Initializing,
    Running,
    ShuttingDown,
    Stopped,
}
```

**Key Methods**:

```rust
impl EbayMcpServer {
    /// Create new server instance
    pub async fn new(config: ServerConfig) -> Result<Self>;

    /// Start the server
    pub async fn run(&mut self) -> Result<()>;

    /// Handle initialization request
    async fn handle_initialize(&self, params: InitializeParams)
        -> Result<InitializeResult>;

    /// Handle tool call request
    async fn handle_tool_call(&self, params: CallToolParams)
        -> Result<CallToolResult>;

    /// Handle resource read request
    async fn handle_resource_read(&self, params: ReadResourceParams)
        -> Result<ReadResourceResult>;

    /// Graceful shutdown
    pub async fn shutdown(&mut self) -> Result<()>;
}
```

### 2. Search Manager Component

**File**: `src/search/manager.rs`

**Responsibilities**:
- Orchestrate search operations
- Manage saved search phrases
- Handle result caching and retrieval
- Track search history

**Key Structures**:

```rust
pub struct SearchManager {
    /// Phrase storage
    phrase_store: Arc<RwLock<PhraseStore>>,

    /// Search executor
    executor: Arc<SearchExecutor>,

    /// Result cache
    cache: Arc<ResultCache>,

    /// History tracker
    history: Arc<Mutex<HistoryTracker>>,

    /// Browser pool reference
    browser_pool: Arc<BrowserPool>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SavedSearchPhrase {
    /// Unique identifier
    pub id: String,

    /// Display name
    pub name: String,

    /// Search query
    pub query: String,

    /// Search filters
    pub filters: SearchFilters,

    /// Tags for organization
    pub tags: Vec<String>,

    /// Creation timestamp
    pub created_at: DateTime<Utc>,

    /// Last used timestamp
    pub last_used: Option<DateTime<Utc>>,

    /// Usage count
    pub usage_count: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchFilters {
    /// Category filter
    pub category: Option<String>,

    /// Price range
    pub price_min: Option<f64>,
    pub price_max: Option<f64>,

    /// Condition (new, used, refurbished)
    pub condition: Option<Vec<String>>,

    /// Buying format (auction, buy_it_now, classified_ads)
    pub buying_format: Option<Vec<String>>,

    /// Location/shipping
    pub location: Option<String>,
    pub shipping: Option<ShippingOptions>,

    /// Sort order
    pub sort_by: Option<SortOrder>,

    /// Item specifics
    pub item_specifics: Option<HashMap<String, String>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SortOrder {
    BestMatch,
    PricePlusShippingLowest,
    PricePlusShippingHighest,
    PriceLowest,
    PriceHighest,
    DistanceNearest,
    TimeEndingSoonest,
    TimeNewlyListed,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ShippingOptions {
    pub free_shipping: bool,
    pub local_pickup: bool,
    pub international: bool,
}
```

**Key Methods**:

```rust
impl SearchManager {
    /// Execute search by query
    pub async fn search(
        &self,
        query: &str,
        filters: Option<SearchFilters>,
    ) -> Result<SearchResults>;

    /// Execute search by saved phrase ID
    pub async fn search_by_phrase_id(&self, id: &str) -> Result<SearchResults>;

    /// Save a new search phrase
    pub async fn save_phrase(&self, phrase: SavedSearchPhrase) -> Result<String>;

    /// Update existing search phrase
    pub async fn update_phrase(&self, id: &str, phrase: SavedSearchPhrase)
        -> Result<()>;

    /// Delete search phrase
    pub async fn delete_phrase(&self, id: &str) -> Result<()>;

    /// List all saved phrases
    pub async fn list_phrases(&self, tags: Option<Vec<String>>)
        -> Result<Vec<SavedSearchPhrase>>;

    /// Get search phrase by ID
    pub async fn get_phrase(&self, id: &str) -> Result<SavedSearchPhrase>;

    /// Get search history
    pub async fn get_history(
        &self,
        limit: usize,
        offset: usize,
    ) -> Result<Vec<SearchHistoryEntry>>;
}
```

### 3. Browser Pool Component

**File**: `src/browser/pool.rs`

**Responsibilities**:
- Manage pool of headless browser instances
- Handle browser lifecycle (create, reuse, destroy)
- Implement connection limits and timeouts
- Monitor browser health

**Key Structures**:

```rust
pub struct BrowserPool {
    /// Pool configuration
    config: BrowserPoolConfig,

    /// Available browser instances
    available: Arc<Mutex<VecDeque<BrowserInstance>>>,

    /// In-use browser instances
    in_use: Arc<Mutex<HashSet<BrowserInstanceId>>>,

    /// Pool statistics
    stats: Arc<RwLock<PoolStats>>,
}

#[derive(Debug, Clone)]
pub struct BrowserPoolConfig {
    /// Minimum pool size
    pub min_size: usize,

    /// Maximum pool size
    pub max_size: usize,

    /// Browser executable path
    pub browser_path: Option<PathBuf>,

    /// Headless mode
    pub headless: bool,

    /// User agent rotation
    pub user_agents: Vec<String>,

    /// Request timeout
    pub timeout: Duration,

    /// Instance max age
    pub max_instance_age: Duration,

    /// Window size
    pub window_size: (u32, u32),
}

pub struct BrowserInstance {
    id: BrowserInstanceId,
    browser: Browser,
    created_at: Instant,
    request_count: AtomicU64,
}

type BrowserInstanceId = Uuid;

#[derive(Debug, Default)]
pub struct PoolStats {
    pub total_created: u64,
    pub total_destroyed: u64,
    pub active_count: usize,
    pub wait_count: usize,
    pub avg_request_duration: Duration,
}
```

**Key Methods**:

```rust
impl BrowserPool {
    /// Create new browser pool
    pub async fn new(config: BrowserPoolConfig) -> Result<Self>;

    /// Acquire browser instance from pool
    pub async fn acquire(&self) -> Result<PooledBrowser>;

    /// Return browser instance to pool
    async fn release(&self, instance: BrowserInstance) -> Result<()>;

    /// Warm up pool to min_size
    pub async fn warm_up(&self) -> Result<()>;

    /// Drain pool (shutdown all instances)
    pub async fn drain(&self) -> Result<()>;

    /// Get pool statistics
    pub fn stats(&self) -> PoolStats;
}

/// RAII wrapper for pooled browser
pub struct PooledBrowser {
    instance: Option<BrowserInstance>,
    pool: Arc<BrowserPool>,
}

impl Drop for PooledBrowser {
    fn drop(&mut self) {
        // Return to pool when dropped
        if let Some(instance) = self.instance.take() {
            tokio::spawn(async move {
                let _ = self.pool.release(instance).await;
            });
        }
    }
}

impl Deref for PooledBrowser {
    type Target = Browser;

    fn deref(&self) -> &Self::Target {
        &self.instance.as_ref().unwrap().browser
    }
}
```

### 4. Scraper Component

**File**: `src/scraper/ebay.rs`

**Responsibilities**:
- Navigate to eBay search pages
- Extract listing data from search results
- Handle pagination
- Parse product details
- Anti-detection measures

**Key Structures**:

```rust
pub struct EbayScraper {
    /// Browser instance
    browser: PooledBrowser,

    /// Scraper configuration
    config: ScraperConfig,

    /// Anti-detection handler
    anti_detection: AntiDetection,
}

#[derive(Debug, Clone)]
pub struct ScraperConfig {
    /// Base eBay URL (for different regions)
    pub base_url: String,

    /// Request delay range (anti-detection)
    pub delay_range: (Duration, Duration),

    /// Max retries on failure
    pub max_retries: u32,

    /// Screenshot on error
    pub screenshot_on_error: bool,

    /// Screenshot directory
    pub screenshot_dir: Option<PathBuf>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchResults {
    /// Search query
    pub query: String,

    /// Applied filters
    pub filters: SearchFilters,

    /// Result listings
    pub items: Vec<EbayListing>,

    /// Total result count
    pub total_count: usize,

    /// Current page
    pub page: usize,

    /// Total pages
    pub total_pages: usize,

    /// Search timestamp
    pub searched_at: DateTime<Utc>,

    /// Time taken to scrape
    pub duration: Duration,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EbayListing {
    /// Item ID
    pub item_id: String,

    /// Title
    pub title: String,

    /// Current price
    pub price: Price,

    /// Shipping cost
    pub shipping: Option<Price>,

    /// Item condition
    pub condition: String,

    /// Buying format
    pub format: BuyingFormat,

    /// Seller information
    pub seller: SellerInfo,

    /// Location
    pub location: String,

    /// Thumbnail URL
    pub thumbnail_url: Option<String>,

    /// Listing URL
    pub listing_url: String,

    /// Number of bids (if auction)
    pub bids: Option<u32>,

    /// Time left (if auction)
    pub time_left: Option<String>,

    /// Free shipping flag
    pub free_shipping: bool,

    /// Returns accepted
    pub returns_accepted: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Price {
    pub amount: f64,
    pub currency: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum BuyingFormat {
    Auction,
    BuyItNow,
    BestOffer,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SellerInfo {
    pub username: String,
    pub feedback_score: u32,
    pub positive_percentage: f64,
}
```

**Key Methods**:

```rust
impl EbayScraper {
    /// Create new scraper instance
    pub async fn new(browser: PooledBrowser, config: ScraperConfig) -> Result<Self>;

    /// Execute search
    pub async fn search(
        &mut self,
        query: &str,
        filters: &SearchFilters,
        page: usize,
    ) -> Result<SearchResults>;

    /// Build search URL
    fn build_search_url(
        &self,
        query: &str,
        filters: &SearchFilters,
        page: usize,
    ) -> String;

    /// Navigate to search page
    async fn navigate_to_search(&mut self, url: &str) -> Result<()>;

    /// Extract listings from current page
    async fn extract_listings(&self) -> Result<Vec<EbayListing>>;

    /// Extract total result count
    async fn extract_total_count(&self) -> Result<usize>;

    /// Handle CAPTCHA (detection and notification)
    async fn handle_captcha(&self) -> Result<()>;

    /// Take screenshot for debugging
    async fn take_screenshot(&self, name: &str) -> Result<PathBuf>;
}
```

### 5. Configuration Manager Component

**File**: `src/config/manager.rs`

**Responsibilities**:
- Load and parse TOML configuration
- Manage saved search phrases file
- Handle configuration updates
- Validate configuration
- Configuration hot-reload support

**Key Structures**:

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AppConfig {
    /// Server configuration
    pub server: ServerConfig,

    /// Browser configuration
    pub browser: BrowserConfig,

    /// Database configuration
    pub database: DatabaseConfig,

    /// Cache configuration
    pub cache: CacheConfig,

    /// Logging configuration
    pub logging: LoggingConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BrowserConfig {
    /// Pool size
    pub pool_min_size: usize,
    pub pool_max_size: usize,

    /// Browser executable
    pub executable_path: Option<PathBuf>,

    /// Headless mode
    pub headless: bool,

    /// User agents for rotation
    pub user_agents: Vec<String>,

    /// Timeouts
    pub page_load_timeout: u64,
    pub element_timeout: u64,

    /// Anti-detection settings
    pub randomize_delay: bool,
    pub delay_min_ms: u64,
    pub delay_max_ms: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SavedPhrasesConfig {
    /// Saved search phrases
    pub phrases: Vec<SavedSearchPhrase>,

    /// Configuration version
    pub version: String,
}

pub struct ConfigManager {
    /// Application config
    config: Arc<RwLock<AppConfig>>,

    /// Saved phrases config
    phrases_config: Arc<RwLock<SavedPhrasesConfig>>,

    /// Config file paths
    config_path: PathBuf,
    phrases_path: PathBuf,

    /// File watcher for hot reload
    watcher: Option<RecommendedWatcher>,
}
```

**Key Methods**:

```rust
impl ConfigManager {
    /// Load configuration from files
    pub async fn load(
        config_path: PathBuf,
        phrases_path: PathBuf,
    ) -> Result<Self>;

    /// Get application config (cloned)
    pub fn get_config(&self) -> AppConfig;

    /// Get saved phrases
    pub async fn get_phrases(&self) -> Vec<SavedSearchPhrase>;

    /// Save search phrase
    pub async fn save_phrase(&self, phrase: SavedSearchPhrase) -> Result<()>;

    /// Update search phrase
    pub async fn update_phrase(
        &self,
        id: &str,
        phrase: SavedSearchPhrase,
    ) -> Result<()>;

    /// Delete search phrase
    pub async fn delete_phrase(&self, id: &str) -> Result<()>;

    /// Write phrases config to disk
    async fn persist_phrases(&self) -> Result<()>;

    /// Enable hot reload
    pub fn enable_hot_reload(&mut self) -> Result<()>;
}
```

### 6. Cache Component

**File**: `src/cache/store.rs`

**Responsibilities**:
- Cache search results
- Implement TTL (time-to-live) expiration
- Handle cache invalidation
- Provide cache statistics

**Key Structures**:

```rust
pub struct ResultCache {
    /// In-memory cache
    memory_cache: Arc<RwLock<HashMap<String, CacheEntry>>>,

    /// Disk cache directory
    cache_dir: PathBuf,

    /// Cache configuration
    config: CacheConfig,
}

#[derive(Debug, Clone)]
pub struct CacheConfig {
    /// Enable caching
    pub enabled: bool,

    /// TTL for cache entries
    pub ttl_seconds: u64,

    /// Max memory cache size (number of entries)
    pub max_memory_entries: usize,

    /// Enable disk cache
    pub enable_disk_cache: bool,

    /// Disk cache directory
    pub disk_cache_dir: PathBuf,
}

struct CacheEntry {
    key: String,
    data: SearchResults,
    created_at: Instant,
    expires_at: Instant,
    access_count: u64,
}
```

**Key Methods**:

```rust
impl ResultCache {
    /// Create new cache
    pub fn new(config: CacheConfig) -> Result<Self>;

    /// Get cached results
    pub async fn get(&self, key: &str) -> Option<SearchResults>;

    /// Store results in cache
    pub async fn set(&self, key: String, results: SearchResults) -> Result<()>;

    /// Invalidate cache entry
    pub async fn invalidate(&self, key: &str) -> Result<()>;

    /// Clear all cache
    pub async fn clear(&self) -> Result<()>;

    /// Cleanup expired entries
    pub async fn cleanup_expired(&self) -> Result<usize>;

    /// Generate cache key
    fn generate_key(query: &str, filters: &SearchFilters) -> String;
}
```

---

## Data Models

### Database Schema (SQLite)

```sql
-- Search history table
CREATE TABLE search_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    query TEXT NOT NULL,
    filters_json TEXT,
    result_count INTEGER,
    searched_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    duration_ms INTEGER,
    success BOOLEAN DEFAULT TRUE,
    error_message TEXT
);

CREATE INDEX idx_search_history_date ON search_history(searched_at DESC);
CREATE INDEX idx_search_history_query ON search_history(query);

-- Saved phrases metadata (augments TOML config)
CREATE TABLE phrase_metadata (
    phrase_id TEXT PRIMARY KEY,
    usage_count INTEGER DEFAULT 0,
    last_used TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Cache metadata
CREATE TABLE cache_entries (
    cache_key TEXT PRIMARY KEY,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP NOT NULL,
    hit_count INTEGER DEFAULT 0,
    size_bytes INTEGER
);

CREATE INDEX idx_cache_expiry ON cache_entries(expires_at);

-- Server metrics
CREATE TABLE metrics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    metric_name TEXT NOT NULL,
    metric_value REAL NOT NULL,
    recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_metrics_name ON metrics(metric_name, recorded_at DESC);
```

### Configuration File Format (TOML)

**`config.toml`** - Main server configuration:

```toml
[server]
name = "ebay-search-mcp"
version = "1.0.0"
log_level = "info"

[browser]
pool_min_size = 2
pool_max_size = 5
executable_path = "/usr/bin/chromium"  # Optional, auto-detect if not set
headless = true
page_load_timeout = 30
element_timeout = 10

# User agent rotation for anti-detection
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36"
]

# Random delays between actions (milliseconds)
randomize_delay = true
delay_min_ms = 500
delay_max_ms = 2000

[database]
path = "./data/ebay_mcp.db"
auto_migrate = true

[cache]
enabled = true
ttl_seconds = 3600  # 1 hour
max_memory_entries = 100
enable_disk_cache = true
disk_cache_dir = "./data/cache"

[logging]
level = "info"
file = "./logs/ebay-mcp.log"
max_file_size = "10MB"
max_backups = 5

[scraper]
base_url = "https://www.ebay.com"
max_retries = 3
screenshot_on_error = true
screenshot_dir = "./screenshots"
```

**`search_phrases.toml`** - Saved search phrases:

```toml
version = "1.0"

[[phrases]]
id = "vintage-cameras"
name = "Vintage Film Cameras"
query = "vintage 35mm film camera"
tags = ["photography", "vintage", "cameras"]
created_at = "2025-11-13T12:00:00Z"

[phrases.filters]
category = "Cameras & Photo"
condition = ["Used", "For parts or not working"]
price_min = 50.0
price_max = 500.0
sort_by = "PricePlusShippingLowest"

[phrases.filters.shipping]
free_shipping = false
local_pickup = true
international = false

[[phrases]]
id = "gaming-laptops"
name = "High-End Gaming Laptops"
query = "gaming laptop RTX 4080"
tags = ["electronics", "computers", "gaming"]
created_at = "2025-11-12T18:30:00Z"

[phrases.filters]
category = "Computers/Tablets & Networking"
condition = ["New", "Open box"]
price_min = 1500.0
price_max = 3000.0
buying_format = ["BuyItNow"]
sort_by = "PriceLowest"

[[phrases]]
id = "rare-vinyl"
name = "Rare Jazz Vinyl Records"
query = "Blue Note jazz vinyl original pressing"
tags = ["music", "vinyl", "jazz", "collectibles"]
created_at = "2025-11-10T09:15:00Z"

[phrases.filters]
category = "Music/Records"
condition = ["Used"]
price_min = 20.0
sort_by = "TimeNewlyListed"

[phrases.filters.shipping]
free_shipping = true
```

---

## MCP Interface Design

### Tools

#### 1. `search_ebay`

**Description**: Search eBay with a custom query and optional filters.

**Input Schema**:
```json
{
  "type": "object",
  "properties": {
    "query": {
      "type": "string",
      "description": "Search query string"
    },
    "filters": {
      "type": "object",
      "description": "Optional search filters",
      "properties": {
        "category": {
          "type": "string",
          "description": "eBay category"
        },
        "price_min": {
          "type": "number",
          "description": "Minimum price"
        },
        "price_max": {
          "type": "number",
          "description": "Maximum price"
        },
        "condition": {
          "type": "array",
          "items": {"type": "string"},
          "description": "Item condition (New, Used, etc.)"
        },
        "buying_format": {
          "type": "array",
          "items": {"type": "string"},
          "description": "Buying format (Auction, BuyItNow, etc.)"
        },
        "sort_by": {
          "type": "string",
          "description": "Sort order"
        },
        "free_shipping": {
          "type": "boolean",
          "description": "Filter for free shipping only"
        }
      }
    },
    "page": {
      "type": "integer",
      "description": "Page number (default: 1)",
      "default": 1
    },
    "max_results": {
      "type": "integer",
      "description": "Maximum results to return (default: 50)",
      "default": 50
    }
  },
  "required": ["query"]
}
```

**Returns**: JSON array of eBay listings with structured data.

**Example Usage**:
```rust
// Tool call handler
async fn handle_search_ebay(args: SearchEbayArgs) -> Result<ToolResult> {
    let results = search_manager
        .search(&args.query, args.filters.as_ref())
        .await?;

    Ok(ToolResult {
        content: vec![TextContent {
            text: serde_json::to_string_pretty(&results)?,
        }],
    })
}
```

#### 2. `search_by_phrase`

**Description**: Execute a search using a saved search phrase by ID.

**Input Schema**:
```json
{
  "type": "object",
  "properties": {
    "phrase_id": {
      "type": "string",
      "description": "ID of saved search phrase"
    },
    "page": {
      "type": "integer",
      "description": "Page number (default: 1)",
      "default": 1
    }
  },
  "required": ["phrase_id"]
}
```

**Returns**: Search results using the saved phrase configuration.

#### 3. `save_search_phrase`

**Description**: Save a new search phrase for later reuse.

**Input Schema**:
```json
{
  "type": "object",
  "properties": {
    "name": {
      "type": "string",
      "description": "Display name for the search phrase"
    },
    "query": {
      "type": "string",
      "description": "Search query string"
    },
    "filters": {
      "type": "object",
      "description": "Search filters (same as search_ebay)"
    },
    "tags": {
      "type": "array",
      "items": {"type": "string"},
      "description": "Tags for organization"
    }
  },
  "required": ["name", "query"]
}
```

**Returns**: Generated phrase ID.

#### 4. `list_search_phrases`

**Description**: List all saved search phrases.

**Input Schema**:
```json
{
  "type": "object",
  "properties": {
    "tags": {
      "type": "array",
      "items": {"type": "string"},
      "description": "Filter by tags"
    }
  }
}
```

**Returns**: Array of saved search phrases with metadata.

#### 5. `update_search_phrase`

**Description**: Update an existing search phrase.

**Input Schema**:
```json
{
  "type": "object",
  "properties": {
    "phrase_id": {
      "type": "string",
      "description": "ID of phrase to update"
    },
    "name": {"type": "string"},
    "query": {"type": "string"},
    "filters": {"type": "object"},
    "tags": {
      "type": "array",
      "items": {"type": "string"}
    }
  },
  "required": ["phrase_id"]
}
```

#### 6. `delete_search_phrase`

**Description**: Delete a saved search phrase.

**Input Schema**:
```json
{
  "type": "object",
  "properties": {
    "phrase_id": {
      "type": "string",
      "description": "ID of phrase to delete"
    }
  },
  "required": ["phrase_id"]
}
```

#### 7. `get_search_history`

**Description**: Retrieve recent search history.

**Input Schema**:
```json
{
  "type": "object",
  "properties": {
    "limit": {
      "type": "integer",
      "description": "Number of results (default: 20)",
      "default": 20
    },
    "offset": {
      "type": "integer",
      "description": "Offset for pagination (default: 0)",
      "default": 0
    }
  }
}
```

#### 8. `clear_cache`

**Description**: Clear the search result cache.

**Input Schema**:
```json
{
  "type": "object",
  "properties": {
    "query_pattern": {
      "type": "string",
      "description": "Optional pattern to match cache keys"
    }
  }
}
```

### Resources

#### 1. `ebay://config`

**URI**: `ebay://config`
**MIME Type**: `application/json`
**Description**: Server configuration and status.

**Returns**:
```json
{
  "server": {
    "name": "ebay-search-mcp",
    "version": "1.0.0",
    "uptime_seconds": 3600
  },
  "browser": {
    "pool_size": 5,
    "active_instances": 2,
    "headless": true
  },
  "cache": {
    "enabled": true,
    "entries": 45,
    "hit_rate": 0.73
  },
  "database": {
    "path": "./data/ebay_mcp.db",
    "size_mb": 2.5
  }
}
```

#### 2. `ebay://phrases`

**URI**: `ebay://phrases`
**MIME Type**: `application/json`
**Description**: All saved search phrases.

**Returns**: Array of `SavedSearchPhrase` objects.

#### 3. `ebay://phrases/{phrase_id}`

**URI**: `ebay://phrases/{phrase_id}`
**MIME Type**: `application/json`
**Description**: Specific search phrase details.

**Returns**: Single `SavedSearchPhrase` object.

#### 4. `ebay://history`

**URI**: `ebay://history`
**MIME Type**: `application/json`
**Description**: Recent search history.

**Returns**: Array of search history entries.

#### 5. `ebay://stats`

**URI**: `ebay://stats`
**MIME Type**: `application/json`
**Description**: Server statistics and metrics.

**Returns**:
```json
{
  "total_searches": 1543,
  "cache_hit_rate": 0.73,
  "avg_search_duration_ms": 2341,
  "most_used_phrases": [
    {"id": "vintage-cameras", "usage_count": 87},
    {"id": "gaming-laptops", "usage_count": 65}
  ],
  "uptime_seconds": 86400
}
```

### Prompts

#### 1. `analyze_ebay_results`

**Description**: Generate a prompt for analyzing eBay search results.

**Arguments**:
- `query`: Search query that was used
- `focus`: Analysis focus (price_trends, seller_reliability, best_deals)

**Output**:
```
Given the following eBay search results for "{query}", please analyze:

1. Price Range Analysis
   - Identify the price distribution
   - Find outliers (unusually high or low prices)
   - Calculate average price

2. Seller Reliability
   - Identify sellers with highest feedback scores
   - Flag any sellers with low ratings
   - Recommend most trustworthy listings

3. Best Value Recommendations
   - Consider price, condition, and shipping
   - Factor in seller reputation
   - Recommend top 3 best value items

4. Market Insights
   - Current market trends
   - Availability assessment
   - Price expectations

Please provide actionable recommendations for the buyer.
```

#### 2. `compare_listings`

**Description**: Generate a comparison prompt for multiple listings.

**Arguments**:
- `item_ids`: Array of eBay item IDs to compare

#### 3. `price_alert_setup`

**Description**: Guide user through setting up a price alert search phrase.

**Arguments**:
- `query`: What they're looking for

---

## Configuration Management

### Configuration Files Location

```
ebay-mcp/
├── config/
│   ├── config.toml              # Main server configuration
│   ├── search_phrases.toml      # Saved search phrases
│   └── .env                     # Environment variables (optional)
├── data/
│   ├── ebay_mcp.db             # SQLite database
│   └── cache/                   # Cached results
│       ├── queries/
│       └── metadata/
├── logs/
│   └── ebay-mcp.log            # Application logs
└── screenshots/                 # Debug screenshots
    └── errors/
```

### Configuration Loading Priority

1. Environment variables (highest priority)
2. Command-line arguments
3. `config.toml` file
4. Default values (lowest priority)

### Environment Variables

```bash
# Server
EBAY_MCP_LOG_LEVEL=debug
EBAY_MCP_CONFIG_PATH=/custom/path/config.toml

# Browser
EBAY_MCP_BROWSER_PATH=/usr/bin/chromium
EBAY_MCP_HEADLESS=true

# Database
EBAY_MCP_DATABASE_PATH=/custom/path/ebay_mcp.db

# Cache
EBAY_MCP_CACHE_ENABLED=true
EBAY_MCP_CACHE_TTL=3600
```

### Hot Reload Support

Configuration changes are detected via file watching:

```rust
impl ConfigManager {
    pub fn enable_hot_reload(&mut self) -> Result<()> {
        let (tx, rx) = channel();
        let mut watcher = notify::recommended_watcher(tx)?;

        watcher.watch(&self.config_path, RecursiveMode::NonRecursive)?;
        watcher.watch(&self.phrases_path, RecursiveMode::NonRecursive)?;

        tokio::spawn(async move {
            while let Ok(event) = rx.recv() {
                if let Ok(Event { kind: EventKind::Modify(_), .. }) = event {
                    // Reload configuration
                    self.reload_config().await?;
                }
            }
        });

        self.watcher = Some(watcher);
        Ok(())
    }
}
```

---

## Headless Browser Integration

### Browser Library: `headless_chrome`

**Why headless_chrome**:
- Pure Rust implementation
- High-level API for Chrome DevTools Protocol
- Active maintenance
- Good performance

**Alternative**: `fantoccini` (WebDriver protocol) if cross-browser support needed.

### Browser Initialization

```rust
use headless_chrome::{Browser, LaunchOptions};

async fn create_browser(config: &BrowserConfig) -> Result<Browser> {
    let mut options = LaunchOptions::default_builder()
        .headless(config.headless)
        .window_size(Some((1920, 1080)))
        .build()?;

    if let Some(path) = &config.executable_path {
        options.path = Some(path.clone());
    }

    // Additional options for anti-detection
    options.args.extend(vec![
        "--disable-blink-features=AutomationControlled".to_string(),
        "--disable-dev-shm-usage".to_string(),
        "--no-sandbox".to_string(), // Only in Docker
        "--disable-setuid-sandbox".to_string(),
        "--disable-web-security".to_string(),
    ]);

    Browser::new(options)
}
```

### Anti-Detection Strategies

1. **User Agent Rotation**:
```rust
async fn set_random_user_agent(&self, page: &Tab) -> Result<()> {
    let ua = self.config.user_agents
        .choose(&mut rand::thread_rng())
        .unwrap();

    page.set_user_agent(ua, None, None).await?;
    Ok(())
}
```

2. **Random Delays**:
```rust
async fn random_delay(&self) {
    let delay = rand::thread_rng()
        .gen_range(self.config.delay_min_ms..=self.config.delay_max_ms);

    tokio::time::sleep(Duration::from_millis(delay)).await;
}
```

3. **Viewport Randomization**:
```rust
async fn set_random_viewport(&self, page: &Tab) -> Result<()> {
    let widths = [1366, 1920, 1440, 1536];
    let heights = [768, 1080, 900, 864];

    let idx = rand::thread_rng().gen_range(0..widths.len());

    page.set_viewport(Viewport {
        width: widths[idx],
        height: heights[idx],
        device_scale_factor: Some(1.0),
        ..Default::default()
    }).await?;

    Ok(())
}
```

4. **JavaScript Fingerprint Masking**:
```rust
async fn inject_anti_detection(&self, page: &Tab) -> Result<()> {
    let script = r#"
        // Override navigator.webdriver
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined
        });

        // Override chrome runtime
        window.chrome = {
            runtime: {}
        };

        // Override permissions
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
                Promise.resolve({ state: Notification.permission }) :
                originalQuery(parameters)
        );
    "#;

    page.evaluate(script, false).await?;
    Ok(())
}
```

### Page Navigation and Scraping

```rust
impl EbayScraper {
    async fn navigate_and_extract(
        &mut self,
        url: &str,
    ) -> Result<SearchResults> {
        let page = self.browser.new_tab()?;

        // Anti-detection setup
        self.set_random_user_agent(&page).await?;
        self.set_random_viewport(&page).await?;
        self.inject_anti_detection(&page).await?;

        // Navigate
        self.random_delay().await;
        page.navigate_to(url)?;
        page.wait_for_element("ul.srp-results")?;

        // Extract data
        let listings = self.extract_listings(&page).await?;
        let total_count = self.extract_total_count(&page).await?;

        Ok(SearchResults {
            items: listings,
            total_count,
            // ... other fields
        })
    }

    async fn extract_listings(&self, page: &Tab) -> Result<Vec<EbayListing>> {
        let script = r#"
            Array.from(document.querySelectorAll('li.s-item')).map(item => ({
                itemId: item.getAttribute('data-iid'),
                title: item.querySelector('.s-item__title')?.textContent,
                price: item.querySelector('.s-item__price')?.textContent,
                shipping: item.querySelector('.s-item__shipping')?.textContent,
                condition: item.querySelector('.s-item__condition')?.textContent,
                location: item.querySelector('.s-item__location')?.textContent,
                listingUrl: item.querySelector('.s-item__link')?.href,
                thumbnailUrl: item.querySelector('.s-item__image img')?.src,
            }));
        "#;

        let result = page.evaluate(script, true).await?;
        let listings: Vec<EbayListing> = serde_json::from_value(result.value)?;

        Ok(listings)
    }
}
```

### Error Screenshot Capture

```rust
async fn handle_scraping_error(&self, error: &Error) -> Result<()> {
    if self.config.screenshot_on_error {
        let timestamp = Utc::now().format("%Y%m%d_%H%M%S");
        let filename = format!("error_{}.png", timestamp);
        let path = self.config.screenshot_dir
            .as_ref()
            .unwrap()
            .join(filename);

        if let Ok(screenshot) = self.browser.current_tab()?.capture_screenshot(
            ScreenshotFormat::PNG,
            None,
            true,
        ) {
            tokio::fs::write(&path, screenshot).await?;
            tracing::error!("Error screenshot saved to: {:?}", path);
        }
    }

    Ok(())
}
```

---

## Search Management System

### Search Phrase Storage

**File-based**: TOML configuration file for human readability and version control.

**Database**: SQLite for metadata (usage count, last used, etc.).

**Hybrid Approach**:
- User-facing data (name, query, filters) in TOML
- Usage statistics and history in SQLite
- Sync between both on updates

### Search Execution Flow

```
┌─────────────────────────────────────────────────────────┐
│ 1. Receive Search Request (query or phrase_id)         │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│ 2. Resolve Search Parameters                           │
│    - If phrase_id: Load from phrase store              │
│    - If query: Use provided filters                    │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│ 3. Check Cache                                          │
│    - Generate cache key from query + filters           │
│    - Check memory cache, then disk cache               │
│    - Return if valid and not expired                   │
└────────────────────┬────────────────────────────────────┘
                     │ Cache miss
                     ▼
┌─────────────────────────────────────────────────────────┐
│ 4. Acquire Browser from Pool                           │
│    - Wait if pool exhausted                            │
│    - Create new instance if under max_size             │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│ 5. Execute Search                                       │
│    - Build eBay search URL                             │
│    - Navigate with headless browser                    │
│    - Extract listing data                              │
│    - Handle errors with retries                        │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│ 6. Process Results                                      │
│    - Parse and validate listing data                   │
│    - Enrich with computed fields                       │
│    - Format for MCP response                           │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│ 7. Update State                                         │
│    - Store in cache                                    │
│    - Update search history                             │
│    - Update phrase usage stats (if phrase_id)          │
│    - Return browser to pool                            │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│ 8. Return Results to MCP Client                        │
└─────────────────────────────────────────────────────────┘
```

### Cache Key Generation

```rust
impl ResultCache {
    fn generate_key(query: &str, filters: &SearchFilters) -> String {
        use sha2::{Sha256, Digest};

        let mut hasher = Sha256::new();
        hasher.update(query.as_bytes());
        hasher.update(serde_json::to_string(filters).unwrap().as_bytes());

        let result = hasher.finalize();
        format!("ebay_{:x}", result)
    }
}
```

---

## Error Handling Strategy

### Error Types

```rust
use thiserror::Error;

#[derive(Error, Debug)]
pub enum EbayMcpError {
    /// Browser-related errors
    #[error("Browser error: {0}")]
    Browser(#[from] headless_chrome::Error),

    /// Configuration errors
    #[error("Configuration error: {0}")]
    Config(String),

    /// Search phrase not found
    #[error("Search phrase not found: {0}")]
    PhraseNotFound(String),

    /// Scraping failed
    #[error("Scraping failed: {0}")]
    ScrapingFailed(String),

    /// Cache errors
    #[error("Cache error: {0}")]
    Cache(#[from] std::io::Error),

    /// Database errors
    #[error("Database error: {0}")]
    Database(#[from] rusqlite::Error),

    /// Serialization errors
    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),

    /// CAPTCHA detected
    #[error("CAPTCHA detected - manual intervention required")]
    CaptchaDetected,

    /// Rate limited
    #[error("Rate limited by eBay")]
    RateLimited,

    /// Network errors
    #[error("Network error: {0}")]
    Network(String),

    /// MCP protocol errors
    #[error("MCP protocol error: {0}")]
    Protocol(String),
}

pub type Result<T> = std::result::Result<T, EbayMcpError>;
```

### Error Recovery Strategies

```rust
impl EbayScraper {
    async fn search_with_retry(
        &mut self,
        query: &str,
        filters: &SearchFilters,
    ) -> Result<SearchResults> {
        let mut attempts = 0;
        let max_retries = self.config.max_retries;

        loop {
            attempts += 1;

            match self.search(query, filters, 1).await {
                Ok(results) => return Ok(results),
                Err(e) => {
                    tracing::warn!(
                        "Search attempt {} failed: {}",
                        attempts,
                        e
                    );

                    if attempts >= max_retries {
                        return Err(e);
                    }

                    match &e {
                        EbayMcpError::CaptchaDetected => {
                            // Don't retry on CAPTCHA
                            return Err(e);
                        }
                        EbayMcpError::RateLimited => {
                            // Exponential backoff
                            let delay = Duration::from_secs(2u64.pow(attempts));
                            tokio::time::sleep(delay).await;
                        }
                        _ => {
                            // Short delay for other errors
                            tokio::time::sleep(Duration::from_secs(1)).await;
                        }
                    }
                }
            }
        }
    }
}
```

### MCP Error Responses

```rust
fn to_mcp_error(error: EbayMcpError) -> McpError {
    match error {
        EbayMcpError::PhraseNotFound(id) => McpError {
            code: -32602,
            message: "Invalid parameter".to_string(),
            data: Some(json!({
                "reason": "Search phrase not found",
                "phrase_id": id,
            })),
        },
        EbayMcpError::CaptchaDetected => McpError {
            code: -32000,
            message: "CAPTCHA detected".to_string(),
            data: Some(json!({
                "reason": "eBay requires manual verification",
                "action": "Please wait and try again later",
            })),
        },
        EbayMcpError::RateLimited => McpError {
            code: -32000,
            message: "Rate limited".to_string(),
            data: Some(json!({
                "reason": "Too many requests to eBay",
                "retry_after": 60,
            })),
        },
        _ => McpError {
            code: -32603,
            message: "Internal error".to_string(),
            data: Some(json!({
                "error": error.to_string(),
            })),
        },
    }
}
```

---

## Security Considerations

### 1. Input Validation

```rust
fn validate_search_query(query: &str) -> Result<()> {
    // Length check
    if query.is_empty() || query.len() > 200 {
        return Err(EbayMcpError::Config(
            "Query must be 1-200 characters".to_string()
        ));
    }

    // SQL injection prevention (even though we use parameterized queries)
    if query.contains("--") || query.contains(";") {
        return Err(EbayMcpError::Config(
            "Invalid characters in query".to_string()
        ));
    }

    Ok(())
}

fn validate_phrase_id(id: &str) -> Result<()> {
    // Must be alphanumeric with hyphens
    if !id.chars().all(|c| c.is_alphanumeric() || c == '-') {
        return Err(EbayMcpError::Config(
            "Invalid phrase ID format".to_string()
        ));
    }

    Ok(())
}
```

### 2. Filesystem Safety

```rust
fn validate_cache_path(path: &Path) -> Result<PathBuf> {
    let cache_dir = PathBuf::from("./data/cache");
    let canonical_cache = cache_dir.canonicalize()?;
    let canonical_path = path.canonicalize()?;

    // Ensure path is within cache directory
    if !canonical_path.starts_with(&canonical_cache) {
        return Err(EbayMcpError::Config(
            "Path outside cache directory".to_string()
        ));
    }

    Ok(canonical_path)
}
```

### 3. Resource Limits

```rust
#[derive(Debug, Clone)]
pub struct ResourceLimits {
    /// Max concurrent searches
    pub max_concurrent_searches: usize,

    /// Max saved phrases
    pub max_saved_phrases: usize,

    /// Max cache size (bytes)
    pub max_cache_size_bytes: u64,

    /// Max database size (bytes)
    pub max_database_size_bytes: u64,

    /// Max search results per query
    pub max_results_per_search: usize,
}

impl Default for ResourceLimits {
    fn default() -> Self {
        Self {
            max_concurrent_searches: 5,
            max_saved_phrases: 1000,
            max_cache_size_bytes: 1024 * 1024 * 1024, // 1GB
            max_database_size_bytes: 100 * 1024 * 1024, // 100MB
            max_results_per_search: 200,
        }
    }
}
```

### 4. Secrets Management

```rust
// Never log sensitive data
fn sanitize_for_logging(url: &str) -> String {
    // Remove any API keys or tokens from URLs
    url.split('?').next().unwrap_or(url).to_string()
}

// Use environment variables for sensitive config
fn load_api_credentials() -> Option<Credentials> {
    env::var("EBAY_API_KEY").ok().map(|api_key| Credentials {
        api_key,
        // Never commit credentials to config files
    })
}
```

### 5. Browser Security

```rust
// Disable unnecessary permissions
let permissions = BrowserPermissions {
    allow_notifications: false,
    allow_geolocation: false,
    allow_microphone: false,
    allow_camera: false,
};

// Isolate browser processes
let browser = Browser::new(LaunchOptions {
    sandbox: true, // Enable sandbox unless in Docker
    disable_gpu: true,
    ..Default::default()
})?;
```

---

## Performance Optimization

### 1. Connection Pooling

Maintain browser pool to avoid repeated initialization:

```rust
impl BrowserPool {
    async fn warm_up(&self) -> Result<()> {
        let mut instances = Vec::new();

        for _ in 0..self.config.min_size {
            let browser = self.create_instance().await?;
            instances.push(browser);
        }

        let mut available = self.available.lock().await;
        available.extend(instances);

        Ok(())
    }
}
```

### 2. Intelligent Caching

```rust
#[derive(Debug, Clone)]
pub struct CacheEntry {
    data: SearchResults,
    created_at: Instant,
    expires_at: Instant,
    access_count: AtomicU64,
    size_bytes: usize,
}

impl ResultCache {
    // LRU eviction
    async fn evict_if_needed(&self) -> Result<()> {
        let cache = self.memory_cache.read().await;

        if cache.len() >= self.config.max_memory_entries {
            drop(cache);

            // Find least recently used
            let mut cache = self.memory_cache.write().await;
            if let Some(lru_key) = cache
                .iter()
                .min_by_key(|(_, entry)| entry.access_count.load(Ordering::Relaxed))
                .map(|(k, _)| k.clone())
            {
                cache.remove(&lru_key);
            }
        }

        Ok(())
    }
}
```

### 3. Parallel Processing

```rust
async fn search_multiple_phrases(
    &self,
    phrase_ids: Vec<String>,
) -> Result<HashMap<String, SearchResults>> {
    let futures: Vec<_> = phrase_ids
        .into_iter()
        .map(|id| async move {
            let results = self.search_by_phrase_id(&id).await?;
            Ok::<_, EbayMcpError>((id, results))
        })
        .collect();

    let results = futures::future::try_join_all(futures).await?;
    Ok(results.into_iter().collect())
}
```

### 4. Lazy Loading

```rust
// Load configuration lazily
pub struct LazyConfig<T> {
    value: OnceCell<T>,
    loader: Box<dyn Fn() -> Result<T> + Send + Sync>,
}

impl<T> LazyConfig<T> {
    pub fn get(&self) -> Result<&T> {
        self.value.get_or_try_init(|| (self.loader)())
    }
}
```

### 5. Database Optimization

```sql
-- Create indexes for common queries
CREATE INDEX idx_search_history_date ON search_history(searched_at DESC);
CREATE INDEX idx_search_history_query ON search_history(query);
CREATE INDEX idx_cache_expiry ON cache_entries(expires_at);

-- Use WAL mode for better concurrency
PRAGMA journal_mode=WAL;

-- Increase cache size
PRAGMA cache_size=-64000;  -- 64MB
```

---

## Technology Stack

### Core Dependencies

**Cargo.toml**:
```toml
[package]
name = "ebay-mcp-server"
version = "1.0.0"
edition = "2021"
authors = ["Your Name"]
license = "MIT"

[dependencies]
# Async runtime
tokio = { version = "1.35", features = ["full"] }
futures = "0.3"

# MCP SDK (when available, or implement protocol)
# mcp-rs = "0.1"  # Hypothetical Rust SDK

# Headless browser
headless_chrome = "1.0"
# Alternative: fantoccini = "0.19"

# HTTP client (for potential API usage)
reqwest = { version = "0.11", features = ["json"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
toml = "0.8"

# Database
rusqlite = { version = "0.30", features = ["bundled"] }
tokio-rusqlite = "0.5"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "json"] }

# Error handling
thiserror = "1.0"
anyhow = "1.0"

# Async channels
tokio-stream = "0.1"
async-channel = "2.1"

# Configuration
config = "0.13"
dotenv = "0.15"

# File watching (hot reload)
notify = "6.1"

# Cryptography (for cache keys)
sha2 = "0.10"

# Date/time
chrono = { version = "0.4", features = ["serde"] }

# Random (for anti-detection)
rand = "0.8"

# UUID
uuid = { version = "1.6", features = ["v4", "serde"] }

# CLI parsing
clap = { version = "4.4", features = ["derive"] }

[dev-dependencies]
# Testing
tokio-test = "0.4"
mockall = "0.12"
criterion = "0.5"
tempfile = "3.8"

[[bench]]
name = "search_bench"
harness = false
```

### Project Structure

```
ebay-mcp-server/
├── Cargo.toml
├── Cargo.lock
├── README.md
├── LICENSE
│
├── config/
│   ├── config.toml
│   ├── search_phrases.toml
│   └── config.example.toml
│
├── src/
│   ├── main.rs
│   ├── lib.rs
│   │
│   ├── server/
│   │   ├── mod.rs
│   │   ├── protocol.rs      # MCP protocol implementation
│   │   ├── transport.rs     # stdio transport
│   │   └── handlers.rs      # Tool/resource handlers
│   │
│   ├── search/
│   │   ├── mod.rs
│   │   ├── manager.rs       # Search orchestration
│   │   ├── executor.rs      # Search execution
│   │   └── filters.rs       # Filter handling
│   │
│   ├── browser/
│   │   ├── mod.rs
│   │   ├── pool.rs          # Browser pool
│   │   ├── instance.rs      # Browser instance
│   │   └── anti_detection.rs
│   │
│   ├── scraper/
│   │   ├── mod.rs
│   │   ├── ebay.rs          # eBay-specific scraping
│   │   ├── extractor.rs     # Data extraction
│   │   └── parser.rs        # HTML parsing
│   │
│   ├── storage/
│   │   ├── mod.rs
│   │   ├── database.rs      # SQLite operations
│   │   ├── cache.rs         # Cache implementation
│   │   └── phrases.rs       # Phrase storage
│   │
│   ├── config/
│   │   ├── mod.rs
│   │   ├── manager.rs       # Config management
│   │   ├── models.rs        # Config structures
│   │   └── validation.rs    # Config validation
│   │
│   ├── models/
│   │   ├── mod.rs
│   │   ├── search.rs        # Search models
│   │   ├── listing.rs       # Listing models
│   │   └── mcp.rs           # MCP types
│   │
│   ├── error.rs             # Error types
│   └── utils/
│       ├── mod.rs
│       ├── logging.rs
│       └── metrics.rs
│
├── tests/
│   ├── integration/
│   │   ├── search_tests.rs
│   │   ├── phrase_tests.rs
│   │   └── cache_tests.rs
│   └── fixtures/
│       └── sample_phrases.toml
│
├── benches/
│   └── search_bench.rs
│
├── examples/
│   ├── simple_search.rs
│   └── phrase_management.rs
│
├── data/                    # Gitignored
├── logs/                    # Gitignored
└── screenshots/             # Gitignored
```

---

## Implementation Plan

### Phase 1: Foundation (Week 1)

**Goals**: Basic project setup and core infrastructure

**Tasks**:
1. Initialize Rust project with Cargo
2. Set up project structure
3. Implement configuration management
   - TOML parsing
   - Environment variable support
   - Validation
4. Set up logging infrastructure
5. Implement basic error types
6. Create database schema and migrations
7. Write integration tests for config loading

**Deliverables**:
- Project compiles
- Config can be loaded and validated
- Database initializes correctly
- Logging works

### Phase 2: Browser Integration (Week 2)

**Goals**: Headless browser setup and basic scraping

**Tasks**:
1. Implement browser pool
   - Instance creation and lifecycle
   - Pool management (acquire/release)
   - Health checks
2. Create scraper module
   - Basic navigation
   - Element extraction
   - Error handling
3. Implement anti-detection measures
   - User agent rotation
   - Random delays
   - JavaScript injection
4. Build eBay search URL generator
5. Parse eBay search results page
6. Unit tests for scraper components

**Deliverables**:
- Browser pool functional
- Can navigate to eBay and extract listings
- Anti-detection working
- Tests passing

### Phase 3: Search Management (Week 3)

**Goals**: Search execution and phrase management

**Tasks**:
1. Implement search manager
   - Search execution logic
   - Result aggregation
   - Error recovery
2. Implement phrase store
   - CRUD operations
   - File persistence (TOML)
   - Database sync
3. Implement history tracker
   - Log searches
   - Query history
4. Implement result cache
   - Memory cache
   - Disk cache
   - TTL and eviction
5. Integration tests for search flows

**Deliverables**:
- Can execute searches end-to-end
- Phrase management working
- Caching functional
- History tracked

### Phase 4: MCP Server (Week 4)

**Goals**: MCP protocol implementation

**Tasks**:
1. Implement MCP protocol layer
   - JSON-RPC handling
   - Message routing
   - Capability negotiation
2. Implement stdio transport
3. Implement all MCP tools
   - search_ebay
   - search_by_phrase
   - save_search_phrase
   - list_search_phrases
   - update_search_phrase
   - delete_search_phrase
   - get_search_history
   - clear_cache
4. Implement MCP resources
   - ebay://config
   - ebay://phrases
   - ebay://history
   - ebay://stats
5. Implement MCP prompts
6. Integration tests with MCP Inspector

**Deliverables**:
- Full MCP server functional
- All tools working
- Resources accessible
- MCP Inspector tests passing

### Phase 5: Polish and Testing (Week 5)

**Goals**: Refinement, optimization, and comprehensive testing

**Tasks**:
1. Performance optimization
   - Profile and optimize hot paths
   - Improve caching strategy
   - Optimize database queries
2. Comprehensive error handling
   - Graceful degradation
   - Retry logic
   - User-friendly error messages
3. Documentation
   - Code documentation
   - User guide
   - Configuration examples
4. Integration testing
   - End-to-end scenarios
   - Edge cases
   - Error scenarios
5. Load testing
   - Concurrent searches
   - Browser pool under load
6. Security audit
   - Input validation
   - Resource limits
   - Secrets management

**Deliverables**:
- Optimized performance
- Comprehensive tests
- Complete documentation
- Security hardened

### Phase 6: Deployment Prep (Week 6)

**Goals**: Prepare for production deployment

**Tasks**:
1. Create Docker container
   - Multi-stage build
   - Optimized image size
   - Health checks
2. CI/CD pipeline
   - Automated tests
   - Build and release
3. Monitoring and observability
   - Metrics export
   - Log aggregation
   - Health endpoints
4. Deployment documentation
   - Installation guide
   - Configuration guide
   - Troubleshooting guide
5. Release preparation
   - Version tagging
   - Changelog
   - Release notes

**Deliverables**:
- Docker image
- CI/CD working
- Deployment docs
- Ready for release

---

## Testing Strategy

### Unit Tests

Test individual components in isolation:

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_phrase_store_crud() {
        let store = PhraseStore::new_temp().await.unwrap();

        let phrase = SavedSearchPhrase {
            id: "test-1".to_string(),
            name: "Test Phrase".to_string(),
            query: "vintage camera".to_string(),
            filters: Default::default(),
            tags: vec!["test".to_string()],
            created_at: Utc::now(),
            last_used: None,
            usage_count: 0,
        };

        // Save
        store.save(phrase.clone()).await.unwrap();

        // Retrieve
        let retrieved = store.get(&phrase.id).await.unwrap();
        assert_eq!(retrieved.name, phrase.name);

        // Update
        let mut updated = phrase.clone();
        updated.name = "Updated Name".to_string();
        store.update(&phrase.id, updated.clone()).await.unwrap();

        let retrieved = store.get(&phrase.id).await.unwrap();
        assert_eq!(retrieved.name, "Updated Name");

        // Delete
        store.delete(&phrase.id).await.unwrap();
        assert!(store.get(&phrase.id).await.is_err());
    }

    #[test]
    fn test_cache_key_generation() {
        let filters = SearchFilters::default();

        let key1 = ResultCache::generate_key("camera", &filters);
        let key2 = ResultCache::generate_key("camera", &filters);
        let key3 = ResultCache::generate_key("laptop", &filters);

        assert_eq!(key1, key2);
        assert_ne!(key1, key3);
    }
}
```

### Integration Tests

Test component interactions:

```rust
#[tokio::test]
async fn test_search_with_caching() {
    let config = load_test_config();
    let server = EbayMcpServer::new(config).await.unwrap();

    let query = "vintage camera";
    let filters = SearchFilters::default();

    // First search (cache miss)
    let start = Instant::now();
    let results1 = server
        .search_manager
        .search(query, Some(filters.clone()))
        .await
        .unwrap();
    let duration1 = start.elapsed();

    // Second search (cache hit)
    let start = Instant::now();
    let results2 = server
        .search_manager
        .search(query, Some(filters))
        .await
        .unwrap();
    let duration2 = start.elapsed();

    // Cache hit should be much faster
    assert!(duration2 < duration1 / 2);

    // Results should be identical
    assert_eq!(results1.items.len(), results2.items.len());
}
```

### End-to-End Tests

Test full MCP flows:

```rust
#[tokio::test]
async fn test_mcp_search_workflow() {
    let mut server = start_test_server().await;

    // Initialize
    let init_request = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "method": "initialize",
        "params": {
            "protocolVersion": "2025-03-26",
            "capabilities": {},
            "clientInfo": {
                "name": "TestClient",
                "version": "1.0.0"
            }
        }
    });

    let init_response = server.handle_request(init_request).await.unwrap();
    assert!(init_response["result"]["capabilities"].is_object());

    // Call search tool
    let search_request = json!({
        "jsonrpc": "2.0",
        "id": 2,
        "method": "tools/call",
        "params": {
            "name": "search_ebay",
            "arguments": {
                "query": "vintage camera",
                "max_results": 10
            }
        }
    });

    let search_response = server.handle_request(search_request).await.unwrap();
    assert!(search_response["result"]["content"].is_array());

    // Verify results
    let results: SearchResults = serde_json::from_value(
        search_response["result"]["content"][0]["text"].clone()
    ).unwrap();

    assert!(!results.items.is_empty());
    assert_eq!(results.query, "vintage camera");
}
```

### Performance Benchmarks

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};

fn bench_search(c: &mut Criterion) {
    let runtime = tokio::runtime::Runtime::new().unwrap();
    let manager = runtime.block_on(async {
        create_search_manager().await.unwrap()
    });

    c.bench_function("search ebay", |b| {
        b.to_async(&runtime).iter(|| async {
            manager
                .search(black_box("camera"), None)
                .await
                .unwrap()
        });
    });
}

criterion_group!(benches, bench_search);
criterion_main!(benches);
```

---

## Deployment Considerations

### Docker Containerization

**Dockerfile**:
```dockerfile
# Build stage
FROM rust:1.75 AS builder

WORKDIR /app

# Cache dependencies
COPY Cargo.toml Cargo.lock ./
RUN mkdir src && \
    echo "fn main() {}" > src/main.rs && \
    cargo build --release && \
    rm -rf src

# Build application
COPY src ./src
RUN cargo build --release

# Runtime stage
FROM debian:bookworm-slim

# Install Chrome dependencies
RUN apt-get update && apt-get install -y \
    chromium \
    chromium-driver \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy binary
COPY --from=builder /app/target/release/ebay-mcp-server ./

# Copy configuration
COPY config ./config

# Create data directories
RUN mkdir -p data logs screenshots

# Set environment
ENV RUST_LOG=info
ENV EBAY_MCP_BROWSER_PATH=/usr/bin/chromium

# Run as non-root
RUN useradd -m -u 1000 mcpuser && \
    chown -R mcpuser:mcpuser /app
USER mcpuser

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD pgrep -x ebay-mcp-server || exit 1

CMD ["./ebay-mcp-server"]
```

### Claude for Desktop Configuration

**macOS** (`~/Library/Application Support/Claude/claude_desktop_config.json`):
```json
{
  "mcpServers": {
    "ebay-search": {
      "command": "/path/to/ebay-mcp-server",
      "args": ["--config", "/path/to/config/config.toml"],
      "env": {
        "RUST_LOG": "info",
        "EBAY_MCP_HEADLESS": "true"
      }
    }
  }
}
```

**Docker-based** (using stdio over Docker):
```json
{
  "mcpServers": {
    "ebay-search": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-v", "/path/to/config:/app/config:ro",
        "-v", "/path/to/data:/app/data",
        "ebay-mcp-server:latest"
      ]
    }
  }
}
```

### System Requirements

**Minimum**:
- CPU: 2 cores
- RAM: 2GB
- Disk: 5GB
- OS: Linux, macOS, or Windows

**Recommended**:
- CPU: 4+ cores
- RAM: 4GB+
- Disk: 10GB+ (for cache)
- OS: Linux or macOS

### Monitoring

**Metrics to track**:
- Search request rate
- Search latency (p50, p95, p99)
- Cache hit rate
- Browser pool utilization
- Error rate
- Database size
- Cache size

**Health endpoint** (for orchestration):
```rust
pub struct HealthStatus {
    pub status: String,  // "healthy" | "degraded" | "unhealthy"
    pub uptime_seconds: u64,
    pub browser_pool: PoolHealth,
    pub database: DatabaseHealth,
    pub cache: CacheHealth,
}

impl EbayMcpServer {
    pub fn health(&self) -> HealthStatus {
        HealthStatus {
            status: if self.is_healthy() { "healthy" } else { "degraded" },
            uptime_seconds: self.uptime().as_secs(),
            browser_pool: self.browser_pool.health(),
            database: self.database.health(),
            cache: self.cache.health(),
        }
    }
}
```

---

## Future Enhancements

### Phase 2 Features (Post-MVP)

1. **Multi-Region Support**
   - Support eBay sites in different countries (ebay.co.uk, ebay.de, etc.)
   - Currency conversion
   - Region-specific filtering

2. **Advanced Filtering**
   - Seller feedback filtering
   - Listing duration
   - Number of watchers
   - Bidding activity

3. **Price Tracking**
   - Track price history for specific items
   - Price drop notifications
   - Price trend analysis

4. **Seller Analysis**
   - Seller reputation scoring
   - Seller inventory analysis
   - Comparative seller statistics

5. **Image Analysis**
   - Extract and cache item images
   - Visual similarity search
   - Image quality assessment

6. **Batch Operations**
   - Bulk search phrase imports/exports
   - Batch search execution
   - Bulk phrase updates

7. **Advanced Caching**
   - Predictive pre-caching of popular searches
   - Incremental cache updates
   - Distributed caching support

8. **API Mode**
   - REST API alongside MCP
   - WebSocket support for real-time updates
   - GraphQL interface

9. **Machine Learning**
   - Price prediction models
   - Fraud detection
   - Recommendation engine

10. **Scheduling**
    - Scheduled search execution
    - Automated alerts
    - Report generation

### Technical Improvements

1. **Performance**
   - Implement distributed browser pool (multiple machines)
   - Add Redis for distributed caching
   - Optimize database with connection pooling

2. **Reliability**
   - Implement circuit breakers
   - Add request deduplication
   - Improve retry strategies

3. **Observability**
   - OpenTelemetry integration
   - Distributed tracing
   - Structured logging with correlation IDs

4. **Testing**
   - Property-based testing
   - Chaos engineering
   - Load testing suite

5. **Documentation**
   - Interactive API documentation
   - Video tutorials
   - Architecture diagrams

---

## Appendices

### Appendix A: eBay Search URL Structure

**Base URL**: `https://www.ebay.com/sch/i.html`

**Query Parameters**:
- `_nkw`: Search query (keywords)
- `_sacat`: Category ID
- `_udlo`: Price min
- `_udhi`: Price max
- `LH_ItemCondition`: Condition (1=New, 3=Used, etc.)
- `LH_BIN`: Buy It Now only (1)
- `LH_Auction`: Auction only (1)
- `LH_FS`: Free shipping (1)
- `_sop`: Sort order (1=Best Match, 15=Price+Shipping Lowest, etc.)
- `_pgn`: Page number

**Example**:
```
https://www.ebay.com/sch/i.html?_nkw=vintage+camera&_sacat=625&_udlo=50&_udhi=500&LH_ItemCondition=3&_sop=15&_pgn=1
```

### Appendix B: DOM Selectors for Scraping

**Search results container**:
```css
ul.srp-results
```

**Individual listing**:
```css
li.s-item
```

**Item data**:
- Title: `.s-item__title`
- Price: `.s-item__price`
- Shipping: `.s-item__shipping`
- Condition: `.s-item__condition`
- Location: `.s-item__location`
- Image: `.s-item__image img`
- URL: `.s-item__link`
- Item ID: `data-iid` attribute

**Result count**:
```css
h1.srp-controls__count-heading
```

### Appendix C: Error Codes

| Code | Name | Description |
|------|------|-------------|
| -32700 | Parse Error | Invalid JSON |
| -32600 | Invalid Request | Invalid JSON-RPC |
| -32601 | Method Not Found | Unknown method |
| -32602 | Invalid Params | Invalid parameters |
| -32603 | Internal Error | Server error |
| -32000 | Server Error | Application error |
| -32001 | Browser Error | Browser failure |
| -32002 | Scraping Error | Scraping failure |
| -32003 | Cache Error | Cache operation failed |
| -32004 | Database Error | Database operation failed |
| -32005 | CAPTCHA Detected | Manual verification required |
| -32006 | Rate Limited | Too many requests |

### Appendix D: Configuration Examples

**Minimal config.toml**:
```toml
[server]
name = "ebay-search-mcp"
version = "1.0.0"

[browser]
headless = true

[database]
path = "./data/ebay_mcp.db"

[cache]
enabled = true
```

**Production config.toml**:
```toml
[server]
name = "ebay-search-mcp-prod"
version = "1.0.0"
log_level = "warn"

[browser]
pool_min_size = 5
pool_max_size = 20
headless = true
page_load_timeout = 30
randomize_delay = true

[database]
path = "/var/lib/ebay-mcp/data.db"
auto_migrate = true

[cache]
enabled = true
ttl_seconds = 7200
max_memory_entries = 500
enable_disk_cache = true
disk_cache_dir = "/var/cache/ebay-mcp"

[logging]
level = "warn"
file = "/var/log/ebay-mcp/server.log"
max_file_size = "50MB"
max_backups = 10
```

---

## Conclusion

This High-Level Design document provides a comprehensive blueprint for implementing an eBay Search MCP Server in Rust. The design emphasizes:

- **Modularity**: Clear separation of concerns across layers
- **Performance**: Browser pooling, caching, and async operations
- **Reliability**: Comprehensive error handling and retry logic
- **Security**: Input validation, resource limits, and safe filesystem operations
- **Maintainability**: Well-structured codebase with extensive testing
- **Extensibility**: Clear paths for future enhancements

### Next Steps

1. Review and approve this design
2. Set up development environment
3. Begin Phase 1 implementation
4. Regular progress reviews
5. Iterative refinement based on testing

### Questions for Stakeholders

1. Are there specific eBay categories or filters to prioritize?
2. What is the expected search volume (requests per hour)?
3. Should we support multiple eBay regions initially or just .com?
4. What is the priority: speed or comprehensiveness of results?
5. Are there specific compliance or legal considerations for scraping?

---

**Document End**

*This document is a living design and will be updated as requirements evolve and implementation progresses.*
