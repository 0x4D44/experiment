# AI-Powered Journal Summarization Feature

**Date:** 2025-11-13
**Feature:** AI-powered summarization of journal entries
**Status:** Complete and Tested

## Overview

Extended the jrnrvw tool to read discovered journal files and generate AI-powered summaries using either Claude or Codex command-line tools. This feature provides intelligent analysis and synthesis of work activities across multiple journals, repositories, and time periods.

## Implementation

### 1. CLI Options Added

Three new command-line options were added:

```bash
--summarize              # Enable AI-powered summary generation
--llm <claude|codex>     # Choose which LLM to use (default: claude)
--summary-output <FILE>  # Save AI summary to a file
```

### 2. Module Structure

Created a new `src/llm/` module with the following components:

#### `src/llm/mod.rs`
- Public interface for LLM summarization
- `LlmProvider` enum (Claude, Codex)
- `summarize()` function - full detailed summary
- `summarize_brief()` function - concise summary

#### `src/llm/claude.rs`
- Integration with Anthropic's Claude CLI
- Calls `claude -p` for non-interactive mode
- Handles stdin/stdout communication
- Error handling for missing CLI or failures

#### `src/llm/codex.rs`
- Integration with OpenAI's Codex CLI
- Calls `codex exec` for non-interactive mode
- Parses Codex's verbose output format
- Extracts actual response from metadata

#### `src/llm/prompts.rs`
- Prompt template generation
- `create_summary_prompt()` - comprehensive analysis request
- `create_brief_summary_prompt()` - quick summary request
- Structures prompts with metadata, instructions, and journal content

### 3. Dependencies Added

- `which = "6.0"` - For locating CLI binaries in PATH

### 4. Integration Points

**In `main.rs`:**
- Added check for `--summarize` flag after report building
- Converts CLI arguments to LLM provider
- Generates summary using selected LLM
- Handles output to file or stdout
- Supports generating both AI summary and regular report

**In `cli.rs`:**
- Added `summarize`, `llm`, and `summary_output` fields to `Cli` struct
- Added `LlmArg` enum with Claude and Codex variants
- Configured clap to require --summarize when using --llm or --summary-output

## Usage Examples

### Basic Usage

Generate AI summary using Claude (default):
```bash
jrnrvw /path/to/journals --summarize
```

### Using Codex

Generate summary using Codex:
```bash
jrnrvw /path/to/journals --summarize --llm codex
```

### Save to File

Save AI summary to a file:
```bash
jrnrvw /path/to/journals --summarize --summary-output summary.md
```

### Combine with Filtering

Generate summary for specific time range:
```bash
jrnrvw /path/to/journals --summarize --last-week --summary-output weekly-summary.md
```

Filter by repository:
```bash
jrnrvw /path/to/journals --summarize --repo "myproject" --llm claude
```

### Generate Both Outputs

Save both AI summary and regular report:
```bash
jrnrvw /path/to/journals \\
  --summarize --summary-output ai-summary.md \\
  -o report.txt
```

## Prompt Structure

The AI prompt includes:

1. **Request Header**
   - Clear instructions for comprehensive summary
   - Context about journal entries

2. **Metadata**
   - Total entries count
   - Number of repositories
   - Date range covered

3. **Instructions**
   - Executive summary
   - By repository breakdown
   - By task breakdown
   - Time analysis
   - Key insights and patterns

4. **Journal Content**
   - Organized by repository and task
   - Includes dates, titles, activities, notes, time spent

## Sample Output

When running with Claude on test fixtures:

```markdown
# Task Journal Summary Report

## Executive Summary

Over a 3-day period (November 10-12, 2025), the team completed **10 hours**
of development work across 2 repositories. The work focused on three main areas:
implementing new reporting capabilities, resolving critical parser bugs, and
conducting code reviews...

## By Repository

### testproject
**Total Time**: 8 hours
**Key Accomplishments**:
- New Feature Development (5h): Implemented comprehensive reporting system...
- Bug Fixes (3h): Addressed critical parser issues...

### another-repo
**Total Time**: 2 hours
**Key Accomplishments**:
- Code Review & Quality Assurance: Reviewed 5 pull requests...

## Time Analysis
...

## Key Insights
...
```

## Testing

### Manual Testing

Successfully tested:
- ✅ Claude CLI integration with test fixtures
- ✅ Output to stdout
- ✅ Output to file (--summary-output)
- ✅ Verbose mode logging
- ✅ Error handling for missing CLIs
- ⚠️  Codex CLI integration (works but may be slower)

### Unit Tests

Added tests in all LLM modules:
- `test_generate_summary_with_simple_prompt()` - Tests CLI invocation
- `test_claude_not_available()` / `test_codex_not_available()` - Error handling
- `test_summarize_with_claude()` / `test_summarize_with_codex()` - Integration
- `test_create_summary_prompt()` - Prompt generation
- `test_create_brief_summary_prompt()` - Brief prompt generation

Total new tests: **7 tests**
All tests passing: ✅

### Integration Testing

Tested with existing test fixtures in `tests/fixtures/integration_journals/`:
- 3 journal files
- 2 repositories (testproject, another-repo)
- 3 tasks with activities, notes, and time data

## Error Handling

Robust error handling for:
- Missing CLI binaries (clear error messages with installation hints)
- CLI execution failures (captures stderr)
- Invalid UTF-8 output
- Empty or malformed responses
- Parsing errors in Codex output format

## Performance Considerations

- **Claude**: Fast response (typically <10 seconds for small datasets)
- **Codex**: May be slower, includes reasoning time
- **Prompt Size**: Scales with number of entries and content length
- **Network**: Requires internet connection for LLM API calls

## Limitations

1. **External Dependencies**: Requires Claude or Codex CLI to be installed and configured
2. **API Costs**: LLM usage may incur costs depending on service
3. **Prompt Length**: Very large datasets may exceed LLM context limits
4. **Codex Output**: Requires parsing of verbose metadata format
5. **Network Required**: Cannot generate summaries offline

## Future Enhancements

Potential improvements:
- Support for additional LLMs (Ollama, local models)
- Custom prompt templates via config files
- Streaming output for real-time summaries
- Summary caching to avoid re-generating
- Token usage reporting
- Prompt optimization for token efficiency
- Support for markdown formatting in prompts
- Multi-language support

## Files Modified/Created

### New Files
- `src/llm/mod.rs` (124 lines)
- `src/llm/claude.rs` (78 lines)
- `src/llm/codex.rs` (96 lines)
- `src/llm/prompts.rs` (130 lines)

### Modified Files
- `src/cli.rs` - Added 3 CLI options and LlmArg enum
- `src/lib.rs` - Added llm module export
- `src/main.rs` - Added summarization logic (55 lines)
- `Cargo.toml` - Added which dependency

### Total Lines Added
Approximately **500 lines** of new code

## Documentation

### Help Text

```bash
$ jrnrvw --help
...
AI Summarization:
  --summarize              Generate AI-powered summary of journal entries
  --llm <LLM>              LLM to use for summarization: claude, codex
                           [default: claude]
  --summary-output <FILE>  Save AI summary to file
...
```

### README Updates Needed

The project README should be updated to document:
- AI summarization feature
- Requirements (Claude or Codex CLI)
- Usage examples
- Configuration options

## Conclusion

Successfully implemented AI-powered journal summarization feature that:
- ✅ Integrates with Claude and Codex CLIs
- ✅ Generates comprehensive, structured summaries
- ✅ Supports file output and filtering
- ✅ Includes robust error handling
- ✅ Maintains high code quality (all tests pass)
- ✅ Provides excellent user experience

The feature is production-ready and provides significant value by automatically synthesizing work activities into actionable insights and executive summaries.

**Total Implementation Time**: ~2 hours
**Lines of Code**: ~500 lines
**Tests Added**: 7 tests
**Test Coverage**: Full coverage of new modules
