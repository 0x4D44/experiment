# High-Level Design: Rust Standalone Benchmarking App

**Date:** 13 Nov 2025  
**Author:** Codex  
**Status:** Draft

## 1. Goals & Scope
- Deliver a portable Rust binary that exercises CPU, memory, disk, I/O, and network subsystems with consistent methodology.
- Produce human-friendly TUI output plus machine-readable JSON for automation.
- Run on Linux/macOS (Phase 1) with future Windows support.
- Keep runtime self-contained (no external dependencies apart from optional iperf peer).

## 2. Non-Goals
- Replacing enterprise-grade monitoring agents (app is synthetic benchmark only).
- Providing deep GPU benchmarking (future extension).
- Persisting long-term metrics (app emits results at end of run).

## 3. Architecture Overview
```
+-------------------------------+
| benchctl (Rust CLI/TUI)       |
+-------------------------------+
| Config Loader | Reporter      |
+-------------------------------+
| Test Orchestrator             |
+-----------------------------------------------+
| CPU | Memory | Disk | File I/O | Network      |
+-----------------------------------------------+
```
- **Config Loader:** Parses CLI/ENV config (scenarios, durations, endpoints).
- **Reporter:** Aggregates metrics, renders TUI via ratatui/crossterm, writes JSON/Markdown.
- **Test Orchestrator:** Schedules tests (parallel/sequential), enforces warmup/cooldown.
- **Test Modules:** Individual components performing measurements with shared utils (timers, histograms, stats).

## 4. Key Components
1. **CLI Layer** (clap): options for enabling/disabling tests, durations, threads, output file path, network server/client mode.
2. **Runtime & TUI**
   - Use tokio for async tasks (network/disk) + std threads for CPU.
   - ratatui-based dashboard showing live metrics, progress bars, pass/fail icons.
3. **Metrics & Stats**
   - Histograms (hdrhistogram crate) for latency/bandwidth.
   - JSON schema for final report with metadata (host info, OS, CPU, memory).
4. **Test Modules**
   - **CPU:** Multi-threaded integer & floating loops, queue depth control, per-core affinity (optional via affinity crate).
   - **Memory:** STREAM-like copy/scale/triad tests using aligned buffers; measure GB/s.
   - **Disk/File I/O:** Sequential & random read/write to temp files (configurable size), fsync toggle.
   - **I/O (syscalls):** Small read/write loops (pipes) to gauge syscall overhead.
   - **Network:** Client/server pair (TCP). Server listens; client sends payloads, measures throughput/latency. Optionally support loopback mode.
5. **Reporting**
   - TUI final summary.
   - JSON output (default `bench_report.json`).
   - Optional Markdown summary.

## 5. Execution Flow
1. Parse config → determine active tests, durations, target endpoints.
2. Collect system metadata (CPU info, mem, OS, disk capabilities) using sysinfo crate.
3. Warm-up per test (configurable, default 5s).
4. Run tests sequentially or parallel per config (default sequential to avoid resource contention).
5. Collect metrics, compute stats (avg, p95, p99, stddev, max, min).
6. Render final TUI screen; write JSON/Markdown.

## 6. Configuration Model
```toml
[general]
output = "bench_report.json"
mode = "sequential" # or parallel
warmup_secs = 5
run_secs = 30

[cpu]
enabled = true
threads = "auto"
operations = ["int", "float"]

[memory]
enabled = true
buffer_mb = 512

[disk]
enabled = true
file_path = "/tmp/bench.dat"
file_size_mb = 1024
fsync = false
pattern = ["seq_read", "seq_write", "rand_read", "rand_write"]

[network]
enabled = true
role = "client" # or server
server_addr = "127.0.0.1:4000"
payload_kb = 64
duration_secs = 30
```

## 7. Module Details
### CPU Test
- Worker threads perform work units (int math, floating math, hashing) with high-resolution timers.
- Report metrics: iterations/sec per thread, aggregated GIPS/GFLOPS.
- Optionally pin threads to cores.

### Memory Test
- STREAM-like kernels (copy/scale/add/triad) using aligned `Vec<f64>`.
- Report GB/s per kernel, cache effects (by varying buffer sizes).

### Disk & File I/O
- Create temp file (configurable path/size) using direct I/O (if supported).
- Sequential read/write throughput, random read/write IOPS.
- Collect latency histograms for random ops.

### I/O/Syscall Test
- Loop on `write/read` with pipes to measure syscall rate.
- Good for detecting kernel overhead or virtualization impact.

### Network Test
- Server: listens on TCP, echoes payload.
- Client: sends payloads concurrently, measures throughput Mbps and latency percentiles.
- Optional UDP mode for packet loss measurement.

## 8. Reporting & UX
- Live dashboard with sections per test (status, current metrics, ETA).
- Summary view with color-coded success/warn thresholds.
- JSON schema: `version`, `host`, `tests` (per test metrics), `warnings`. Example snippet:
```json
{
  "version": "0.1.0",
  "host": {"os": "Linux", "cpu": "AMD EPYC", "memory_gb": 64},
  "tests": {
    "cpu": {"gflops": 320.5, "threads": 16},
    "memory": {"stream_copy_gbps": 220.1},
    ...
  },
  "drift": {"baseline_file": "metrics/baselines/...", "psi": {"latency": 0.02}}
}
```

## 9. Packaging & Distribution
- Cargo workspace with binary `benchctl`.
- Cross-platform builds via GitHub Actions (Linux/macOS). Windows support tracked separately.
- Provide static binary releases (musl) for Linux.

## 10. Implementation Phases
1. **Phase 0 (2 weeks):** Scaffold CLI, CPU/memory tests, JSON output.
2. **Phase 1 (2-3 weeks):** Add disk/I/O/network modules, TUI, baseline exporter integration.
3. **Phase 2:** Add remote coordination (server/client automation), plugin system, Kubernetes job templates.

## 11. Risks & Mitigations
- **Permissions:** Disk/raw I/O may need elevated perms → detect and warn.
- **Hardware diversity:** Provide capability detection & skip unsupported tests.
- **Network dependencies:** Provide loopback mode for offline runs.

## 12. Open Questions
- Should network test auto-spawn server thread or rely on external peer?
- Do we need GPU benchmarks in future releases?
- How to integrate results with existing Python KPI pipeline (e.g., import JSON outputs)?
