# Multistage Implementation Plan — Rust Standalone Benchmarking App
**Date:** 13 Nov 2025  
**Source:** `2025.11.13 - HLD - rust benchmarking app.md`  
**Goal:** Deliver the benchctl binary described in the HLD through incremental, testable stages that de-risk concurrency, cross-platform I/O, and reporting.

## Revision History
| Date | Version | Notes |
| --- | --- | --- |
| 13 Nov 2025 | v0.2 | Review-driven updates: concurrency ADR, staged networking rollout, baseline alignment, risk register. |
| 13 Nov 2025 | v0.1 | Initial draft derived directly from HLD. |

## Review Adjustments (13 Nov 2025)
- **Concurrency & Orchestration Clarity:** Stage 0 now owns a runtime ADR and blocking/async integration tests so later modules inherit a single execution model.
- **Baseline Workflow Alignment:** Stage 1 captures reference runs only; automated drift gating moves to Stage 3 where reporting/baseline tooling ships.
- **Stage 2 Scope Split:** Storage/syscall work (Wave 2a) and network/concurrency work (Wave 2b) ship sequentially to limit hardware contention and enable focused soak tests.
- **Remote Coordination & Cleanup Guarantees:** Multi-host harnesses, artifact hygiene, and teardown validation are explicit deliverables before exiting Stage 2b.
- **Risk Tracking:** Added dependency-aware stage table and a central risk register to surface cross-functional blockers earlier.

---
## 1. Guiding Principles
- Ship vertical slices that exercise config → orchestrator → reporter end-to-end each stage.
- Treat reproducibility as a feature: every task includes determinism, cleanup, and validation.
- Prefer feature flags over long-lived branches so benchmarking modules can be toggled independently.
- Maintain parity between TUI and JSON outputs at every milestone to unblock automation consumers.
- Lock runtime/concurrency decisions early (Stage 0 ADR) so later modules target a single execution model.

## 2. Planning Assumptions
1. Linux (x86_64) and macOS (arm64/x86_64) are mandatory in Stages 0–2; Windows unblocked later.
2. GitHub Actions is available for CI, including nightly `tokio` + `musl` toolchains.
3. Optional iperf/remote peers exist but cannot be relied on for automated tests; loopback mode must be default.
4. Engineering capacity ≈ 2 FTE; durations below assume parallel work when feasible.

## 3. Stage Overview
| Stage | Focus | Duration* | Primary Deliverables | Key Dependencies / Notes |
| --- | --- | --- | --- | --- |
| 0 | Project Foundations & Telemetry Spine | 2 weeks | CLI/config loader, orchestrator skeleton, JSON schema, system metadata probe | Runtime ADR + config schema must be approved before Stage 1 starts. |
| 1 | CPU & Memory Benchmarks + Basic Reporting | 3 weeks | CPU/memory modules, warmup/cooldown logic, histogram stats, TUI stub | Depends on Stage 0 schema + orchestrator contracts; cross-platform CI lanes go green here. |
| 2 | Storage, Syscall, and Network Workloads (Wave 2a: storage/syscall, Wave 2b: network/concurrency) | 4 weeks | Disk & I/O modules, network client/server, concurrency policy, artifact hygiene | Requires Stage 1 telemetry stability; lab hardware reserved for disk soak (2a) and multi-host network tests (2b). |
| 3 | Baselines, Automation, & Packaging | 3 weeks | Drift comparison, Markdown export, remote coordination hooks, release pipeline | Needs Stage 2 network harness + cleanup guarantees; becomes first CI-gated release. |
| 4 (stretch) | Windows + Plugin Ecosystem | 4+ weeks | Windows port, plugin API, Kubernetes/job templates | Triggered after Stage 3 release; depends on Windows build agents + plugin ADR. |

\*Durations are calendar estimates with 1-week overlap for hardening between stages.

## 4. Stage Details
### Stage 0 – Project Foundations & Telemetry Spine (Weeks 1–2)
**Objectives**
- Create `benchctl` workspace, config model, and orchestrator contracts.
- Establish logging, metrics collection, and JSON serialization pipeline used by all later modules.

**Tasks**
1. Scaffold cargo workspace, clap-based CLI, config loader (global + per-module defaults).
2. Implement orchestrator state machine covering lifecycle hooks (init → warmup → run → cooldown → report) with trait-based module interface.
3. Add system metadata collector (sysinfo) and host fingerprint block in JSON.
4. Define JSON schema + versioning; emit placeholder metrics for stub modules.
5. Set up CI (lint, fmt, unit tests) and artifact signing strategy for nightly binaries.
6. Author ADR covering async runtime vs blocking thread strategy, including fallback when tokio is unavailable.
7. Implement smoke tests that run dummy modules through orchestrator on both Linux & macOS builders to validate the ADR assumptions.

**Exit Criteria**
- Running `benchctl --dry-run` produces JSON with metadata and empty test sections.
- Orchestrator contract reviewed; module template compiled for CPU/memory.
- Runtime/concurrency ADR approved by stakeholders; smoke tests pass on both target platforms.

**Risks/Mitigations**
- *Risk:* Unclear concurrency boundaries → hold ADR capturing tokio vs std thread strategy.
- *Risk:* Config sprawl → introduce schema validation (schemars) before Stage 1.

### Stage 1 – CPU & Memory Benchmarks + Basic Reporting (Weeks 3–5)
**Objectives**
- Deliver first real workloads and hook live metrics into the TUI/JSON pipeline.

**Tasks**
1. Implement CPU worker pool (int/float/hashing) with affinity option + per-thread stats.
2. Build memory STREAM kernels with configurable buffer sizes and stride profiles.
3. Integrate hdrhistogram; compute avg/p95/p99/stddev per workload.
4. Finish warmup/cooldown enforcement + sequential/parallel scheduling toggle.
5. Stand up ratatui dashboard (status cards, progress bars) plus passthrough JSON writer.
6. Add golden-run tests that compare deterministic output hashes for small buffers.
7. Expand CI matrix (Linux x86_64, macOS arm64) to exercise CPU/memory suites in release and debug builds with perf budget alerts.

**Exit Criteria**
- `benchctl --tests cpu,memory` runs end-to-end on Linux & macOS; JSON + TUI show identical aggregate metrics.
- Reference runs are captured on two hardware profiles and stored as manual artifacts for future baseline comparison (no automated gating yet).

**Risks/Mitigations**
- *Risk:* Mixed tokio/std threads causing contention → document CPU pinning defaults, allow `--no-affinity` override.
- *Risk:* Memory benchmarks thrash caches → support buffer configs in TOML with validation.
- *Risk:* macOS CI runners lag Linux capacity → keep lightweight smoke suite per PR and defer full perf runs to nightly schedule.

### Stage 2 – Storage, Syscall, and Network Workloads (Weeks 6–9)
**Objectives**
- Expand coverage to disk, file I/O, syscall loops, and network throughput/latency.
- Finalize orchestrator concurrency policy, multi-host coordination, and artifact hygiene.
- Deliver in two waves to keep feedback loops short and avoid cross-workstream contention.

**Wave 2a – Storage & Syscall Focus (Weeks 6–7)**
1. Disk module: sequential & random patterns, configurable sizes, fsync toggle, direct-I/O capability detection with buffered fallback.
2. Temp-file manager service that enforces capacity quotas, isolates per-run directories, and guarantees cleanup even on panic.
3. Syscall module: pipe-based read/write loop with tunable payload bytes, concurrency, and tracing spans for latency attribution.
4. Orchestrator resource tagging (disk, cpu, memory, network) plus guardrails that prevent conflicting modules from co-running.
5. Expand TUI with per-module warning banners for throttling, disk cleanup status, and syscall saturation hints.

**Wave 2b – Network & Concurrency Hardening (Weeks 8–9)**
1. Network module: built-in TCP server thread, client worker pool, loopback default, external peer handshake, and retry/backoff policy.
2. Multi-host harness: GitHub Actions workflow (or lab script) that spins up paired runners, coordinates ports, and archives traces.
3. Failure-injection scenarios (server unavailable, packet loss) with deterministic error messaging and recovery.
4. Concurrency policy hardened: document and enforce max parallel modules, plus CLI flags for overriding when needed.
5. Artifact hygiene verification: automated tests assert no temp files or sockets remain after cancellation or crash.

**Exit Criteria**
- Wave 2a: Disk and syscall modules runnable via config selection; orchestrator refuses conflicting parallel runs; temp artifacts are deleted automatically.
- Wave 2b: Network tests pass in loopback mode by default and via external peer harness; failure modes are surfaced in TUI/JSON with actionable messages.
- Nightly multi-host harness publishes bandwidth/latency trends; issues block promotion to Stage 3.

**Risks/Mitigations**
- *Risk:* macOS lacks O_DIRECT → detect support, fall back to buffered I/O with warning (already part of Wave 2a Task 1).
- *Risk:* Limited lab hardware for soak tests → reserve slots during Stage 1 exit review and support synthetic CI loopback jobs as fallback.
- *Risk:* Network flakiness → include retry/timeouts + capture tcpdump snapshots in harness for debugging.

### Stage 3 – Baselines, Automation, & Packaging (Weeks 10–12)
**Objectives**
- Turn the tool into a repeatable CI artifact with drift detection and richer outputs.

**Tasks**
1. Define baseline file format + commands (`benchctl baseline export|compare`).
2. Implement drift math (percent delta, PSI) and surface results in JSON/TUI warnings.
3. Produce Markdown summary exporter for human-readable sharing.
4. Wire GitHub Actions: matrix builds (Linux/macos), musl static release, artifact notarization where needed.
5. Add remote coordination hooks (e.g., simple REST/gRPC or SSH runner spec) for orchestrating client/server pairs.
6. Author operator docs covering config, prerequisites, and troubleshooting.
7. Execute end-to-end multi-host tests inside CI (two-runner workflow) and in lab hardware to validate remote orchestration reliability.
8. Create release readiness checklist (schema version bump, baseline refresh, SBOM, documentation) enforced via CI gate.

**Exit Criteria**
- Releases generated via CI with attached JSON schema version and changelog.
- Baseline comparisons gate CI (fail on regression thresholds) and show up in reports; release is blocked until new baselines are archived.
- Remote coordination validated both locally (auto-spawn server) and across two distinct machines/runners with recorded artifacts.

**Risks/Mitigations**
- *Risk:* Baseline drift noise → allow smoothing windows + manual override thresholds.
- *Risk:* Release signing complexity → document manual fallback process before GA.

### Stage 4 – Windows & Plugin Ecosystem (Stretch, Weeks 13+)
**Objectives**
- Extend platform support and open plugin surface for future GPU/advanced tests.

**Tasks**
1. Abstract OS-specific layers (affinity, temp files, perf counters) for Windows.
2. Port networking and disk modules; ensure overlapped I/O compatibility.
3. Define plugin API (trait objects + dynamic loading) and Sample GPU placeholder plugin.
4. Provide Kubernetes/job templates invoking benchctl in containerized environments.

**Exit Criteria**
- Windows CI pipeline green; plugins documented with sample crate.

## 5. Cross-Cutting Workstreams
- **Quality & Testing:** unit tests per module, integration scenarios per stage, nightly soak runs on reference hardware.
- **Observability:** structured logging (tracing) with module IDs, optional `--log-json` flag for log aggregation.
- **Security & Compliance:** dependency audit (cargo-deny), binary SBOM generation prior to release.
- **Documentation:** living README, per-stage runbooks, and HLD deltas captured as ADRs.

## 6. Checkpoints & Governance
1. **Stage Kickoff Reviews:** validate scope/success criteria before coding.
2. **Demo Days (bi-weekly):** run live benchmarks on shared hardware; capture regressions.
3. **Go/No-Go Gates:** each stage exit requires updated JSON schema versioning and docs sign-off.
4. **Backlog Hygiene:** triage open questions from HLD (network peer automation, GPU scope) by Stage 2 midpoint.

## 7. Dependencies & Resource Matrix
| Workstream | Primary Owner | Support | Tooling |
| --- | --- | --- | --- |
| Orchestrator & Runtime | Core Rust dev | Ops | tokio, tracing |
| CPU/Memory Modules | Perf engineer | QA | affinity, hdrhistogram |
| Disk/IO Modules | Systems engineer | SRE | direct I/O probes, fio baselines |
| Network Module | Systems engineer | Infra | inbuilt server, optional iperf |
| Reporting/Baselines | Tooling dev | Data eng | ratatui, serde_json |

## 8. Appendices
- **A. Terminology:** Warmup (time before measurements), Cooldown (time after run ensuring flush), Drift (delta vs baseline median).
- **B. Outstanding Decisions:** external network peer strategy, GPU roadmap alignment, KPI pipeline integration timeline.

## 9. Risk Register & Contingencies
| ID | Risk | Impact | Response | Trigger |
| --- | --- | --- | --- | --- |
| R1 | Concurrency ADR delayed or disputed | Blocks Stage 1 development | Escalate via architecture review within Stage 0; fall back to documented single-threaded orchestrator until resolved. | ADR not approved by end of Stage 0 Week 1. |
| R2 | Lab hardware unavailable for disk/network soak | Slips Stage 2 validation dates | Reserve slots during Stage 1 exit review; maintain loopback CI jobs as degraded-mode acceptance. | Hardware reservation rejected or revoked. |
| R3 | Baseline noise causes false CI failures | Release pipeline stalls in Stage 3 | Add smoothing windows, manual override flag, and requirement to capture dual reference hosts before enabling gating. | Consecutive nightly runs show >5% variance. |
| R4 | Windows agent provisioning blocked | Stage 4 stretch slips indefinitely | Capture decision by end of Stage 3; if agents unavailable, split Stage 4 into plugin work (Linux) and Windows backlog. | No Windows builder access confirmed by Stage 3 exit. |
